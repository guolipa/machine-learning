![](https://i.loli.net/2019/01/10/5c37362b1e6f7.jpg)
# 模型评估与选择  

## 误差与过拟合

分类器分类错误的样本数占样本总数的比例为错误率(errorate)，E=m/n(m为分类错误的总样本数，n为总样本数），分类的精度(accuracy) P=(1-E)x100% 

学习器实际预测值和样本真实值之间的差异为“误差”

- **训练误差/经验误差：**学习器在训练数据上的误差
- **泛华误差：**学习器在新样本上的误差

我们希望得到的是泛华误差尽可能小的学习器，在通过训练数据训练学习器的过程中是在尽可能的最小化训练误差。有时学习器的训练误差非常小，但当遇到新的样本时表现的却不好。因此为了的当泛华能力比较强的学习器，只能让学习器从训练样本中学习到尽可能适应所有样本的普遍规律，要尽可能的学习所有训练样本的共性，这样学习器在遇到引得样本时才会做出正确的判断。通过训练数据训练出的学习器会有两个问题：

- **过拟合:**学习器学习能力过于强大把训练数据学习的太好了，可能把训练样本自身的特点当做了所有样本的一般性质，导致过拟合，是学习器的泛化能力降低。过拟合无法避免，只能尽量减小。
- **欠拟合：**和过拟合相对应，有学习器学习能力过低，对训练样本的一般性质没有学好，欠拟合容易克服，关键的问题是过拟合  

## 模型评估和选择方法

不同的机器学习算法，同一算法的不同参数配置都是不同的模型，机器学习的模型选择就是对模型的泛化误差进行评估选择泛化误差最小的模型。

通过训练数据得到的学习器，将学习器在在测试数据上的测试误差作为学习器的泛华误差。   

训练集与测试集要有如下关系：训练集和测试集都是从样本真实分布中独立同分布采样而得，他们应该尽可能互斥，测试样本尽量不出现在训练样本中。

假设包含m个样例的样本集```D={(x1,y1),(x2,y2),...,(xm,ym)}```,如何划分训练集和测试集的常用方法如下：      

1. **留出法**   
直接将样本集D划分为两个互斥的集合D= S *U* T,S作为训练集，T作为测试集估计分类器的泛化误差。      
	- 训练集和测试集的划分要保持数据分布的一致性，避免数据划分引入偏差影响结果。比如在分类任务中要保持两个集合中样本的类别别比例相似。
	- 在使用留出法时不同的划分会得到不同的模型，单次留出法得到的结果往往不够稳定可靠，一般采用若干次留出法随机划分，重复试验评估后取平均值作为留出法的评估结果。并且常将样本集的2/3 ~ 4/5的样本用于训练，剩余的用于测试。

2. **交叉验证法**   
	- 将样本集D划分为k个大小相似的互斥子集，D=D1 *U* D2 *U*...*U* Dk,每个子集尽可能保持数据分布的一致性，   
	- 每次使用k-1个子集的并集作为训练集，剩余的一个子集作为测试集，    
	- 这样可以得到k组训练/测试集，进行k次实验，得到的是k次测试结果的均值      
	                      	 
	交叉验证法评估结果的稳定性很大程度取决于k的取值，因此也称为“k折交叉”验证，k常取10，其他常用值为5，20等。   
	
	**交叉验证法的特例：留一法(LOO)，k=m**
	- 留一法的优点：留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集，每个子集一个样本，在多数情况下，留一法中被实际评估的模型和期望评估的用D训练出的模型很相似，评估结果往往被认为很准确。
	- 留一法的缺点：数据集比较大时，计算复杂度高，开销非常大。
3. **自助法**   
自助法采用又放回采样方法，原始样本集D，每次从D中随机挑选一个样本，拷贝到D'中，并放回D中，重复每次，就得到了包含m个样本的数据集D'。   
通过采样法，初始数据集D中有约36.8%的样本未出现在D'中，D'作为训练集，D/D'作为测试集    

**总结：**

- 在初始数据集比较充足时，留出法和交叉验证法更常用一些
- 在数据集比较小，难以有效划分训练/测试集是使用自助法
- 对于数据集小且可有效划分的时候最好使用留一法来进行划分，因为这种方法最为准确  

**数据集的分类及作用**

- 训练集 training set： 用于训练模型   
- 验证集 validation set： 用于模型选择    
- 测试集 test set： 用于最终对学习方法的评估 

## 调参  

机器学习中的两类参数：

- 算法的参数/超参数
- 模型的参数

算法的参数往往在实数范围内，因此对每种参数配置都训练出模型是不可能的，现实中往往对每个参数选定一个范围和步长值，这样选定的参数不是最佳的，但这是计算开销和性能估计之间进行这种的结果。调参往往很苦难，工程量大。

## 性能度量
衡量模型泛化能力的评价标准就是性能度量，对比不同模型时使用不同的性能度量往往得到不同的评判结果。所以模型好坏是相对的，不仅取决于算法和数据，还决定于任务需求。
### 错误率和精度












