
# 一. 概率知识  

- **先验概率**：先验概率（prior probability）是指根据以往经验和分析得到的概率

- **后验概率**：后验概率,事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小
后验概率就是条件概率

		p(c|x) = p(x|c)p(c)/p(x)  

		贝叶斯概率引入先验知识和逻辑推理来处理不确定的命题

- 事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。  

- 先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识

# 二. 贝叶斯分类器基本原理  

## 1. 贝叶斯判定准则 

假设训练集为 ```T = {(x1,y1),(x2,y2),....,(xN,yN)}```, 有 K 中标记类型，标记集 ```y = {c1,c1,...,cK}```     

定义损失函数 ```L(Y=ck,f(x))``` 为将样本 x 分类错误所产生的损失  
![1.png](https://i.loli.net/2019/01/20/5c446e1ad188a.png)   
  
- 基于后验概率可以得到将样本 x 分类为 ci 产生的**期望损失**，即在样本x上的‘**条件风险**’     
  
	![2.png](https://i.loli.net/2019/01/20/5c446e1b18a9c.png)  

	>注：期望损失也称为风险      

- 对于分类准则f(x),总体风险为：     

	![3.png](https://i.loli.net/2019/01/20/5c446e1b571d4.png)    
 
我们的任务就是找到一个分类准则使总体风险最小化。如果对于每个样本 x ，若f(x)能最小化条件风险 R(f(x)|x) ,则总体风险 R(f) 也将最小化。

- **贝叶斯判定准则： 为了最小化总体风险，只需在每个样本上选择那个能使条件风险R(c|x)最小的类别标记**       
 
	![5.png](https://i.loli.net/2019/01/20/5c446e1b93d00.png)  

上式为最小分类错误的贝叶斯最优分类器，即对每个样本x，选择能使后验概率 P(c | x)最大的类别标记，所以**期望风险最小化准则就是后验概率最大化准则**

## 2. 朴素贝叶斯

## 2.1 朴素贝叶斯的原理

经过上面的分析我们得到要想设计最优的贝叶斯判定准则来最小化决策风险，首先要获得的就是后验概率 P(c| x)。  

由贝叶斯定理可以得到后验概率的计算公式：  

![1.png](https://i.loli.net/2019/01/20/5c447664b2e5e.png)  

P( c ) 是先验该概率，P(x | c)是样本 x 相对于类标记 c 的条件概率，或称为似然概率  

- 先验概率p(c)表示样本空间中各类样本所占的比例
- 类条件概率p(x | c)是样本x所有属性的联合概率，难以从有限的训练样本中直接估计得到，为了避开这个障碍简化问题，**朴素贝叶斯法**做了**‘ 属性条件独立的 ’**的假设，即对已知类别，**假设所有属性相互独立**。 

这就是朴素贝叶斯的由来，朴素贝叶斯分类器中的**’ 朴素 ‘**的含义是： **所有属性特征相互独立同等重要**。如果属性之间不相互独立就是贝叶斯网，一种经典的概率图模型。

有朴素贝叶斯分类器的的属性相互独立的假设条件可以将后验概率公式变为如下：  

![2.png](https://i.loli.net/2019/01/20/5c447664f0258.png)  

由于对于所有类别来说 P(x) 相同，因此可以得到贝叶斯判定准则如下，xi 为样本 x 第 i 个属性的值：  

![3.png](https://i.loli.net/2019/01/20/5c44766568b9e.png)

这就是**朴素贝叶斯分类器的表达式**。

所以朴素贝叶斯分类器学习的关键是如何求解 P(c) 和 P(x | c)

## 2.2 极大似然估计法  

>初学，目前还没有搞懂，先占个坑吧！

在朴素贝叶斯算法中，分类器的训练学习意味着估计 P(c) 和 P(xi | c) ,可以使用极大似然估计法估计响应的概率，先验概率和条件概率的极大似然估计如下：  

![微信图片_20190124153603.png](https://i.loli.net/2019/01/24/5c496afa95f04.png)

## 2.3 朴素贝叶斯分类算法  

**朴素贝叶斯分类器的训练过程就是基于训练集来估计类先验概率 P(c) , 和每个属性的条件概率 P(xi | c)**

![1547991856(1).png](https://i.loli.net/2019/01/20/5c447b52d0501.png) 

>**算法实例** 

![TIM图片20190123165332.png](https://i.loli.net/2019/01/24/5c4965bf5ba7a.png)  

![TIM图片20190123165345.png](https://i.loli.net/2019/01/24/5c4965bfa054b.png)
 
>**注：算法和实例图来自于《统计学方法》**

## 2.5 贝叶斯估计

**实现算法中可能出现的问题**  

- **下溢出问题**

	训练朴素贝叶斯分类器之后，当有新样本时，会计算样本属于某个类别的概率   

	```P(i) = P(ci) P(w0 | ci) P(w1 | ci) ... P(wk | ci) ```    

	在计算概率是由于太多很小的数相乘，程序会下溢出或者得不到正确答案(比如python程序在乘法中得到非常小的输时会四舍五入) 
	
	一种解决方法是对上面的乘法计算公式取对数将乘法转化为加法，```ln(a*b) = ln(a) + ln(b)```,通过对数避免下溢出或四舍五入，采用对数进行处理不会有任何损失，不会影响最终结果(因为算法最后是通过比较各个类别概率大小决定样本的类别，而不是返回概率本身)  

- **训练集中特征不存在，概率为零** 
 
	在计算条件概率时，如果某个属性在训练集中没有和某个类同事出现，那么这个条件概率就为0，这样在最后计算样本属于该类的概率时不管其他属性如何，这个概率都为零，为了避免其他属性携带的信息被训练集中未出现的属性抹去，通常要进行**“平滑”**，常用拉普拉斯平滑。

**贝叶斯估计**

用极大似然估计会出现所要计算的概率为0的情况，会影响到后验概率的计算进而影响最终的判定结果，使分类产生偏差。解决这一问题的方法是使用**贝叶斯估计**代替极大似然估计，下面公式是先验概率和条件概率的贝叶斯估计：

![微信图片_20190124153610.png](https://i.loli.net/2019/01/24/5c49771b49aa2.png)

**λ ≧ 0 ，当 λ = 0 时，就是最大似然估计；当 λ = 1 时， 就是拉普拉斯平滑**

## 2.5 朴素贝叶斯分类器的实现方式

- 基于高斯分布模型

	```GuassianNB```:高斯朴素贝叶斯算法是基于假设特征服从高斯分布
	
- 基于贝努利模型

	```MultinomialNB```:用于实现服从多项式分布数据的朴素贝叶斯算法，常用于文本分类(文本数据经常用词向量来表示)

- 基于多项式模型

	```BernoulliNB```:用于实现服从多变量贝努利分布数据的朴素贝叶斯算法，这种算法要求样本表示成二进制特征向量    





